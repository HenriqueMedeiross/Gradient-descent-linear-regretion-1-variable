# Gradient-descent-linear-regretion-1-variable

Implementation of the gradient descent algorithm for linear regretion with 1 variable in python.

The hypothesis function that we want to achieve/predict:

<img src="https://github.com/HenriqueMedeiross/Gradient-descent-linear-regretion-1-variable/blob/master/eq1.png?raw=true" width="30%" height="30%">

where i is the index of the sample

The mean square error function J is as follows:

<img src="https://github.com/HenriqueMedeiross/Gradient-descent-linear-regretion-1-variable/blob/master/eq2.png?raw=true" width="30%" height="30%">

To minimize the error function J we use the gradient descent <img src="https://github.com/HenriqueMedeiross/Gradient-descent-linear-regretion-1-variable/blob/master/eq3.png?raw=true" width="10%" height="10%">:

<img src="https://github.com/HenriqueMedeiross/Gradient-descent-linear-regretion-1-variable/blob/master/eq4.png?raw=true" width="30%" height="30%">

<img src="https://github.com/HenriqueMedeiross/Gradient-descent-linear-regretion-1-variable/blob/master/eq5.png?raw=true" width="30%" height="30%">

In this script I used the numpy libreary that makes a lot easyer to make this vector calculations.
a
