# Gradient-descent-linear-regretion-1-variable

Implementation of the gradient descent algorithm for linear regretion with 1 variable in python.

The hypothesis function that we want to achieve/predict:


![eq1](https://github.com/HenriqueMedeiross/Gradient-descent-linear-regretion-1-variable/blob/master/eq1.png?raw=true){:class="img-responsive"}

where i is the index of the sample

The mean square error function J is as follows:

![eq2](https://github.com/HenriqueMedeiross/Gradient-descent-linear-regretion-1-variable/blob/master/eq2.png?raw=true)

To minimize the error function J we use the gradient descent ![eq3](https://github.com/HenriqueMedeiross/Gradient-descent-linear-regretion-1-variable/blob/master/eq3.png?raw=true):

![eq4](https://github.com/HenriqueMedeiross/Gradient-descent-linear-regretion-1-variable/blob/master/eq4.png?raw=true)

![eq5](https://github.com/HenriqueMedeiross/Gradient-descent-linear-regretion-1-variable/blob/master/eq5.png?raw=true)

In this script I used the numpy libreary that makes a lot easyer to make this vector calculations.
a
